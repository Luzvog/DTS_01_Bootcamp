{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedimiento:\n",
    "\n",
    "Entrar al directorio con el archivo .sh\n",
    "```txt\n",
    "cd /home/airman/Escritorio/Repos_HENRY/DTS_01_Bootcamp/.DS-M4-Cluster_Hadoop\n",
    "\n",
    "```\n",
    "Verificar directorio\n",
    "```\n",
    "dir (muestra los archivos en el directorio)\n",
    "dir -ll (muestra los archivos en el directorio, sus metadatos y permisos)\n",
    "\n",
    "```\n",
    "output\n",
    "\n",
    "```\n",
    "total 12\n",
    "drwxrwxr-x 2 airman airman 4096 may 15 17:00 config\n",
    "-rw-rw-r-- 1 airman airman 1700 may 15 17:00 README.md\n",
    "-rw-rw-r-- 1 airman airman  877 may 15 17:00 start-container.sh\n",
    "\n",
    "```\n",
    "\n",
    "para poder ejercutar el archivo start-container es necesario conferirle permisos de ejecucion\n",
    "\n",
    "ejecutamos\n",
    "```\n",
    "chmod 777 start-container.sh\n",
    "```\n",
    "\n",
    "output\n",
    "```\n",
    "total 12\n",
    "drwxrwxr-x 2 airman airman 4096 may 15 17:00 config\n",
    "-rw-rw-r-- 1 airman airman 1700 may 15 17:00 README.md\n",
    "-rwxrwxrwx 1 airman airman  877 may 15 17:00 start-container.sh\n",
    "\n",
    "```\n",
    "1. Crear red para el cluster de hadoop\n",
    "\n",
    "sudo docker network create --driver=bridge hadoop\n",
    "\n",
    "2. Inicializar el cluster\n",
    "\n",
    "sudo ./start-container.sh\n",
    "\n",
    "output:\n",
    "\n",
    "start hadoop-master container...\n",
    "start hadoop-slave1 container...\n",
    "start hadoop-slave2 container...\n",
    "root@hadoop-master:~# \n",
    "\n",
    "dir (muestra los archivos en dfs)\n",
    "\n",
    "output\n",
    "```\n",
    "root@hadoop-master:~# dir\n",
    "hdfs  run-wordcount.sh\tstart-hadoop.sh\n",
    "\n",
    "```\n",
    "\n",
    "3. Iniciar hadoop\n",
    "./start-hadoop.sh\n",
    "\n",
    "Verificar directorio\n",
    "\n",
    "```\n",
    "dir -ll (muestra los archivos en el directorio, sus metadatos y permisos)\n",
    "```\n",
    "output:\n",
    "drwxr-xr-x 1 root root 4096 Jun  5  2020 hdfs\n",
    "-rwxr-xr-x 1 root root  695 Jun  1  2020 run-wordcount.sh\n",
    "-rwxr-xr-x 1 root root  120 Jun  1  2020 start-hadoop.sh (ya cuenta con permisos de ejecución )\n",
    "\n",
    "-- Revisar el localhost:8088 para ir al cluster hadoop\n",
    "\n",
    "4. Un archivo txt de un libro (descargamos y lo guardamos en el cluste)\n",
    "wget https://raw.githubusercontent.com/uracilo/testdata/master/odisea.txt\n",
    "\n",
    "5. Crear un directorio (creamos un directorio llamado input)\n",
    "mkdir input\n",
    "\n",
    "\n",
    "6. Crear un archivo tipo tar.gz (formato de compresion de archivos)\n",
    "\n",
    "tar -czvf input/odisea.tar.gz odisea.txt\n",
    "\n",
    "en este punto se creo el tar.gz y se guardo en la carpeta input\n",
    "\n",
    "-c: Generar archivo -z: Comprimir con gzip. -v: Progreso del proceso. -f: Especificar nombre del archivo.\n",
    "\n",
    "7. Revisar los tamaños de nuestros archivos\n",
    "ls -flarts input\n",
    "Output\n",
    "```\n",
    "total 292\n",
    "  8 drwx------ 1 root root   4096 May 16 00:02 ..\n",
    "  4 drwxr-xr-x 2 root root   4096 May 16 00:04 .\n",
    "280 -rw-r--r-- 1 root root 284744 May 16 00:04 odisea.tar.gz\n",
    "\n",
    "``` \n",
    "\n",
    "8. Crear y mover directorio input al DFS de HADOOP\n",
    "\n",
    "hdfs dfs - (hace referencia al sistema de archivos del cluster)\n",
    "\n",
    "hdfs dfs -mkdir -p test\n",
    "hdfs dfs -put input\n",
    "\n",
    "9. Revisar nuestro input directorio en HADOOP\n",
    "hdfs dfs –ls  input\n",
    "\n",
    "Output\n",
    "```\n",
    "Found 2 items\n",
    "drwxr-xr-x   - root supergroup          0 2022-05-16 00:20 /user/root/input\n",
    "drwxr-xr-x   - root supergroup          0 2022-05-16 00:20 /user/root/test\n",
    "\n",
    "\n",
    "``` \n",
    "\n",
    "10. Leer las primeras lineas de nuestro archivo en HADOOP\n",
    "\n",
    "-cat (investigar)\n",
    "| (se utiliza para ejecutar multiples comando)\n",
    "-zcar (descomprime el archivo)\n",
    "-tail -n 20 cuenta las primeras 20 lineas \n",
    "\n",
    "hdfs dfs -cat  input/odisea.tar.gz | zcat | tail -n 20\n",
    "\n",
    "11. Eliminar el archivo en HADOOP\n",
    "hdfs dfs –rm –f /user/rawdata/example/odisea.tar.gz\n",
    "\n",
    "Plus ejecutar un trabajo en HADOOP\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.7.2-sources.jar org.apache.hadoop.examples.WordCount input output\n",
    "\n",
    "Plus ver el resultado del trabajo en HADOOP\n",
    "hdfs dfs -cat output/part-r-00000 (resultado del conteo de palabras)\n",
    "\n",
    "salir del docker \n",
    "\n",
    "-abrir una nueva terminal y ejecutar:\n",
    "sudo docker ps -a\n",
    "\n",
    "Output\n",
    "```\n",
    "CONTAINER ID   IMAGE            COMMAND                  CREATED             STATUS                   PORTS                                                                                      NAMES\n",
    "c62160e20827   uracilo/hadoop   \"sh -c 'service ssh …\"   About an hour ago   Up About an hour                                                                                                    hadoop-slave2\n",
    "79e954de1b32   uracilo/hadoop   \"sh -c 'service ssh …\"   About an hour ago   Up About an hour                                                                                                    hadoop-slave1\n",
    "68ec89918e76   uracilo/hadoop   \"sh -c 'service ssh …\"   About an hour ago   Up About an hour         0.0.0.0:8088->8088/tcp, :::8088->8088/tcp, 0.0.0.0:50070->50070/tcp, :::50070->50070/tcp   hadoop-master\n",
    "56081a03218f   hello-world      \"/hello\"                 2 hours ago         Exited (0) 2 hours ago    \n",
    "```\n",
    "\n",
    "-detener un contenedor: \n",
    "sudo docker stop c62160e20827\n",
    "\n",
    "sudo docker stop 79e954de1b32\n",
    "\n",
    "sudo docker stop 68ec89918e76\n",
    "\n",
    "-verificamos status de los contenedores: sudo docker ps -a\n",
    "\n",
    "Output\n",
    "```\n",
    "CONTAINER ID   IMAGE            COMMAND                  CREATED             STATUS                            PORTS     NAMES\n",
    "c62160e20827   uracilo/hadoop   \"sh -c 'service ssh …\"   About an hour ago   Exited (137) About a minute ago             hadoop-slave2\n",
    "79e954de1b32   uracilo/hadoop   \"sh -c 'service ssh …\"   About an hour ago   Exited (137) About a minute ago             hadoop-slave1\n",
    "68ec89918e76   uracilo/hadoop   \"sh -c 'service ssh …\"   About an hour ago   Exited (137) 24 seconds ago                 hadoop-master\n",
    "56081a03218f   hello-world      \"/hello\"                 2 hours ago         Exited (0) 2 hours ago                      goofy_jepsen\n",
    "```\n",
    "\n",
    "-remover los contenedores:\n",
    "sudo docker rm c62160e20827  \n",
    "\n",
    "sudo docker rm 79e954de1b32\n",
    "\n",
    "sudo docker rm 68ec89918e76\n",
    "\n",
    "-verificamos el estatus: sudo docker ps -a\n",
    "\n",
    "Output\n",
    "```\n",
    "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n",
    "```\n",
    "\n",
    "-detener red: sudo docker network ls\n",
    "\n",
    "Output\n",
    "```\n",
    "NETWORK ID     NAME      DRIVER    SCOPE\n",
    "23ba8096c2fb   bridge    bridge    local (por defecto, siempre aparece)\n",
    "90bddf54a168   hadoop    bridge    local\n",
    "f5caa435456d   host      host      local (por defecto, siempre aparece)\n",
    "e57cb08ba928   none      null      local (por defecto, siempre aparece)\n",
    "\n",
    "```\n",
    "\n",
    "-remover la red: sudo docker network rm 90bddf54a168\n",
    "\n",
    "-verificar redes\n",
    "\n",
    "Output\n",
    "```\n",
    "NETWORK ID     NAME      DRIVER    SCOPE\n",
    "23ba8096c2fb   bridge    bridge    local (por defecto, siempre aparece)\n",
    "f5caa435456d   host      host      local (por defecto, siempre aparece)\n",
    "e57cb08ba928   none      null      local (por defecto, siempre aparece)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "root@hadoop-master:~# sistema de archivos del nodo maestro\n",
    "\n",
    "hdfs dfs (sistema de archivos remoto, se accede a los archivos de los demas servidores del cluster)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
